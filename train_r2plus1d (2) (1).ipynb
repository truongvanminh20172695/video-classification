{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_r2plus1d (2).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6V4_-qzoR67s","executionInfo":{"status":"ok","timestamp":1636894749897,"user_tz":240,"elapsed":57057,"user":{"displayName":"Nhật Trần Hoàng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeW7zedaLpvucQ2wCWyhjc2FAasnmpmxQt0WKA=s64","userId":"06916812096908105427"}},"outputId":"5bde5152-d9f0-4bae-a573-572860138cf2"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qF5GT6jdRvbT","executionInfo":{"status":"ok","timestamp":1636894752469,"user_tz":240,"elapsed":587,"user":{"displayName":"Nhật Trần Hoàng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeW7zedaLpvucQ2wCWyhjc2FAasnmpmxQt0WKA=s64","userId":"06916812096908105427"}},"outputId":"3d168f7f-6f91-415f-f11a-cb6bd95b0e1b"},"source":["# Đổi đường dẫn này thành đường dẫn đến thư mục r(2+1)D trong drive của chú\n","%cd /content/drive/MyDrive/Anh_Qui\n","%ls"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Anh_Qui\n","data_train.csv  data_val.csv  label.txt  \u001b[0m\u001b[01;34moutputs\u001b[0m/  \u001b[01;34mTrain\u001b[0m/  \u001b[01;34mVal\u001b[0m/\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ugORq4KnGR_Z","executionInfo":{"status":"ok","timestamp":1636894760715,"user_tz":240,"elapsed":3532,"user":{"displayName":"Nhật Trần Hoàng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjeW7zedaLpvucQ2wCWyhjc2FAasnmpmxQt0WKA=s64","userId":"06916812096908105427"}},"outputId":"c0aeb1f3-0626-484e-cea5-774a515e23e2"},"source":["%pip install torch-summary"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch-summary\n","  Downloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\n","Installing collected packages: torch-summary\n","Successfully installed torch-summary-1.4.5\n"]}]},{"cell_type":"code","metadata":{"id":"rABRp3O8T0xl"},"source":["import os\n","import cv2\n","import time\n","\n","import argparse\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from PIL import Image\n","\n","import torch \n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import albumentations as A\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","from torch.utils.data import DataLoader, Dataset\n","from torchsummary import summary\n","\n","\n","# Thay đổi giá trị default của các argument, hạn chế thay đổi code bên dưới\n","ap = argparse.ArgumentParser()\n","ap.add_argument('-f')\n","ap.add_argument('--model_file', default='outputs/checkpoints/model_TSNsss.pth',\n","                help='path to save the trained model.')\n","ap.add_argument('--data_dir_train_0', default='Train/0',\n","                help='path to data folder.')\n","ap.add_argument('--data_dir_train_1', default='Train/1',\n","                help='path to data folder.')\n","ap.add_argument('--data_dir_val_0', default='Val/0',\n","                help='path to data folder.')\n","ap.add_argument('--data_dir_val_1', default='Val/1',\n","                help='path to data folder.')\n","ap.add_argument('--data_file_train', default='data_train.csv',\n","                 help='path to data file.')\n","ap.add_argument('--data_file_val', default='data_val.csv',\n","                help='path to data file.')\n","ap.add_argument('--label_file', default='label.txt',\n","                help='path to label file.')\n","ap.add_argument('--output_dir', default='outputs',\n","                help='path to output folder.')\n","\n","ap.add_argument('--test_size', type=float, default=0.2,\n","                help='percentage of test set.')\n","ap.add_argument('--epochs', type=int, default=10,\n","                help='number of epochs to train our network for.')\n","ap.add_argument('--batch_size', type=int, default=32)\n","ap.add_argument('--lr', type=float, default=1e-3)\n","ap.add_argument('--save_frequency', type=int, default=1,\n","                help='number of epochs between each checkpoint.')\n","args = ap.parse_args()\n","\n","# check cuda\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'Device: {device}')\n","\n","# get clip and label\n","# df = pd.read_csv(args.data_file)\n","# clips = df['clip'].tolist()\n","\n","# # change string type to array type\n","# def alter_type(clip):\n","#     clip = clip.replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\")\n","#     clip = clip.replace(\"/content/drive/MyDrive/Colab Notebooks/frames_temp\", args.data_dir)\n","#     clip = clip.split(\", \")\n","#     return clip\n","\n","# # process all clip_len frames to get X\n","# def get_clip_input(clips):\n","#     return [alter_type(clip) for clip in clips]\n","def load_clip_data(data_dir, data_file):\n","    # get clip and label\n","    df = pd.read_csv(data_file)\n","    clips = df['clip'].tolist()\n","\n","    # change string type to array type\n","    def alter_type(clip):\n","        clip = clip.replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\")\n","        clip = clip.replace(\"/content/drive/MyDrive/Colab Notebooks/frames_temp\", data_dir)\n","        clip = clip.split(\", \")\n","        return clip\n","\n","    # process all clip_len frames to get X\n","    def get_clip_input(clips):\n","        return [alter_type(clip) for clip in clips]\n","    \n","    X = get_clip_input(clips)\n","    y = df['label'].tolist()\n","    return X, y\n","\n","with open(args.label_file, 'r') as f:\n","    class_names = [_.strip() for _ in f.readlines()]\n","\n","# # get X, y\n","# X = get_clip_input(clips)\n","# y = df['label'].tolist()\n","# print('X:', len(X))\n","# print('y:', len(y))\n","\n","X_RGB, y = load_clip_data(args.rgb_data_dir, args.rgb_data_file)\n","X_OF, y = load_clip_data(args.of_data_dir, args.of_data_file)\n","print('Xs:', len(X_RGB), len(X_OF))\n","print('y:', len(y))\n","\n","# train, test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=args.test_size, random_state=42)\n","print(f'Training instances: {len(X_train)}')\n","print(f'Validataion instances: {len(X_test)}')\n","print()\n","\n","# custom dataset\n","class UCF11(Dataset):\n","    def __init__(self, clips, targets, transform=None, dtype=torch.float32, device='cpu'):\n","        self.clips = clips\n","        self.targets = targets\n","        self.labels = np.unique(self.targets).tolist()\n","        self.target2id = lambda _: self.labels.index(_)\n","        self.y = torch.tensor([self.target2id(_) for _ in self.targets],\n","                              dtype=torch.long)\n","        self.transform = transform\n","        self.dtype = dtype\n","        self.device = device\n","\n","    def __len__(self):\n","        return len(self.clips)\n","\n","    def __getitem__(self, i):\n","        clip = self.clips[i]\n","        input_frames = []\n","        for frame in clip:\n","            image = Image.open(frame)\n","            image = image.convert('RGB')\n","            image = np.array(image)\n","            if self.transform is not None:\n","                image = self.transform(image=image)['image']\n","            input_frames.append(image)\n","        input_frames = np.asarray(input_frames)\n","        # print('input_frames.shape: ', input_frames.shape)\n","        # input_frames = np.expand_dims(input_frames, axis=0)\n","        input_frames = np.transpose(input_frames, (3, 0, 1, 2))\n","        input_frames = torch.tensor(input_frames,\n","                                    dtype=self.dtype,\n","                                    device=self.device)\n","        # label\n","        y = self.y[i].to(self.device)\n","        return (input_frames, y)\n","\n","transform = A.Compose([\n","    A.Resize(128, 171, always_apply=True),\n","    A.CenterCrop(112, 112, always_apply=True),\n","    A.Normalize(mean=[0.43216, 0.394666, 0.37645],\n","                std=[0.22803, 0.22145, 0.216989],\n","                always_apply=True)\n","])\n","\n","train_set = UCF11(X_train, y_train, transform=transform, device=device)\n","val_set = UCF11(X_test, y_test, transform=transform, device=device)\n","\n","train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True)\n","val_loader = DataLoader(val_set, batch_size=args.batch_size, shuffle=True)\n","\n","# model\n","model = torchvision.models.video.r2plus1d_18(pretrained=True, progress=True)\n","model.fc = nn.Linear(model.fc.in_features, len(class_names))\n","model = model.to(device)\n","# summary(model, (3, 16, 112, 112))\n","summary(model, (3, 8, 112, 112))\n","print()\n","\n","# criterion\n","criterion = nn.CrossEntropyLoss()\n","\n","# optim\n","optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\n","# scheduler\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer,\n","    mode='min',\n","    factor=0.5,\n","    patience=5,\n","    min_lr=1e-6,\n","    verbose=True\n",")\n","\n","# training\n","def fit(model, train_loader):\n","    model.train()\n","    train_running_loss = 0.0\n","    train_running_correct = 0\n","    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n","    for i, (X, y) in pbar:\n","        optimizer.zero_grad()\n","        outputs = model(X)\n","        preds = outputs.argmax(1)\n","\n","        loss = criterion(outputs, y)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        train_running_loss += loss.item()\n","        train_running_correct += (preds == y).sum().item()\n","        pbar.set_description(f'[Training iter {i+1}/{len(train_loader)}] batch_loss={loss.item():.03f}')\n","    train_loss = train_running_loss / len(train_loader.dataset)\n","    train_accuracy = 100. * train_running_correct / len(train_loader.dataset)\n","    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}')\n","    return train_loss, train_accuracy\n","\n","# validating\n","@torch.no_grad()\n","def validate(model, test_loader):\n","    model.eval()\n","    val_running_loss = 0.0\n","    val_running_correct = 0\n","    pbar = tqdm(enumerate(test_loader), total=len(test_loader))\n","    for i, (X, y) in pbar:\n","        outputs = model(X)\n","        preds = outputs.argmax(1)\n","\n","        loss = criterion(outputs, y)\n","        val_running_loss += loss.item()\n","        val_running_correct += (preds == y).sum().item()\n","\n","        pbar.set_description(f'[Validation iter {i+1}/{len(train_loader)}] batch_loss={loss.item():.03f}')\n","    val_loss = val_running_loss / len(test_loader.dataset)\n","    val_accuracy = 100. * val_running_correct / len(test_loader.dataset)\n","    print(f'Val loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}')\n","    return val_loss, val_accuracy\n","\n","\n","train_loss, train_accuracy = [], []\n","val_loss, val_accuracy = [], []\n","start = time.time()\n","start_epoch = 0\n","\n"," # resume from previous states\n","#  len() to check file_name \n","if len(args.model_file) and os.path.exists(args.model_file):\n","    print(f'Loading checkpoint from {args.model_file}')\n","    state_dicts = torch.load(args.model_file, map_location=device)\n","    start_epoch = state_dicts['epoch']\n","    model.load_state_dict(state_dicts['model_state_dict'])\n","    optimizer.load_state_dict(state_dicts['optimizer_state_dict'])\n","    scheduler.load_state_dict(state_dicts['scheduler_state_dict'])\n","\n","    train_loss = state_dicts['train_loss']\n","    train_accuracy = state_dicts['train_accuracy']\n","    val_loss = state_dicts['val_loss']\n","    val_accuracy = state_dicts['val_accuracy']\n","    # del state_dicts là để xóa tên biến \n","    del state_dicts\n","\n","for epoch in range(start_epoch, args.epochs):\n","    print(f\"[Epoch {epoch+1} / {args.epochs}]\")\n","\n","    train_epoch_loss, train_epoch_accuracy = fit(model, train_loader)\n","    val_epoch_loss, val_epoch_accuracy = validate(model, val_loader)\n","\n","    # print(f'[Epoch {epoch+1} / {args.epochs}] Training summary: train_loss={train_loss:.3f}, train_acc={train_accuracy:.3f}')\n","    # print(f'[Epoch {epoch+1} / {args.epochs}] Validation summary: val_loss={val_loss:.3f}, val_acc={val_accuracy:.3f}')\n","\n","    train_loss.append(train_epoch_loss)\n","    train_accuracy.append(train_epoch_accuracy)\n","    val_loss.append(val_epoch_loss)\n","    val_accuracy.append(val_epoch_accuracy)\n","    scheduler.step(val_epoch_loss)\n","\n","    # save state dicts after each epoch\n","    if len(args.model_file) and ((epoch > 0 and epoch % args.save_frequency == 0) or epoch == args.epochs - 1):\n","        os.makedirs(os.path.dirname(args.model_file), exist_ok=True)\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scheduler_state_dict': scheduler.state_dict(),\n","            'train_loss': train_loss,\n","            'train_accuracy': train_accuracy,\n","            'val_loss': val_loss,\n","            'val_accuracy': val_accuracy,\n","            }, args.model_file)\n","        print(f'\\t[Epoch {epoch+1} / {args.epochs}] Model checkpointed at {args.model_file}')\n","    print()\n","\n","end = time.time()\n","print(f'TRAINING COMPLETED! Total elapsed time: {(end-start)/60:.3f} minutes')\n","\n","# accuracy plots\n","plt.figure(figsize=(10, 7))\n","plt.plot(train_accuracy, color='green', label='train accuracy')\n","plt.plot(val_accuracy, color='blue', label='validataion accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.grid()\n","plt.legend()\n","plt.savefig(os.path.join(args.output_dir, 'accuracy_train_TSN.png'))\n","\n","# loss plots\n","plt.figure(figsize=(10, 7))\n","plt.plot(train_loss, color='orange', label='train loss')\n","plt.plot(val_loss, color='red', label='validataion loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.grid()\n","plt.legend()\n","plt.savefig(os.path.join(args.output_dir, 'loss_train_TSN.png'))\n","\n","# serialize the model to disk\n","print('Saving model...')\n","torch.save(model.state_dict(), os.path.join(args.output_dir, 'r2plus1d__TSN_OF.pth'))"],"execution_count":null,"outputs":[]}]}